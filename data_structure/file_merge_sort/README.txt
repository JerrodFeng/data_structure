1.问题思考过程：
（1）.当你在电话中问我这个问题时，我首先想到的是位图，因为它和我在读《编程珠玑》时一个问题很想似（1.海量数据2.内存受限），并且当时我想到若是存在数据大小限制可以采用桶排序，所以我当时，向你问了一下它的其他信息；
但是因为数据是double，而且没说不重复，所以位图pass掉了，同理桶排序不合适；
所以当时只是记得应该批量处理。。。。。。
（2）.对于基于磁盘的归并排序：
归并排序思路：首先，对未排序文件（eg:data.txt）进行一次遍历，批量将数据读出，在内存中排序后，写到缓存文件(eg:datax.txt(x = 1,2,3....))中,此时每次读取数据的大小取决的内存大小，得到排序好的很多小文件；然后利用多路归并排序，同时读取所有小文件，依次取每个文件中的一个数放到一个数组中，则第一次取出的为所有文件中最小的，然后将这些最小的集合中的最小数据写到目标文件(result.txt)中，同时在相应的文件中取出一个数据补充到数组中，以此类推可以将数据排序。（当然此过程可以优化：利用败者树、置换-选择排序、最佳归并树，但是由于我马上要考试和课程设计，就没实现）

2.时间复杂度分析:
总共消耗的时间 = 内部排序（产生初始归并段）所需时间 + 外存信息读取时间 + 内部归并所需时间
内部排序所需时间取决于所采用的排序算法，时间复杂度不确定（在程序中我采用的是基于DSU的自带的sort方法（用c实现的归并））
外存信息读取时间为：为进行两趟读/写时间（其实若是不采用一次性归并完成，其归并趟数会变化）
内部归并所需时间：若以两个值之间的比较为单位值，时间复杂度为o(nlogn)(n为总的数据数)
其中外存信息读取时间占主要因素

3.关于代码
请先运行:python generate_data.py,然后运行:python merge_sort.py

generate_data.py 用于产生1000个随机浮点数
merge_sort.py 用于利用磁盘归并排序进行排序
在其中假设内存最多能放下100个python浮点数对象，因为python中一切皆为对象，则每个浮点数实际占用内存不确定，而且python是弱数据类型语言，我只是想表达归并排序的意思

4.mongodb的使用,我当时用mongodb很简单,就是利用python的pymongo模块，因为bson数据格式和dict很相似，能将dict直接存进去，就将python对象存进去了或将自定义对象利用pickle存进去的，当时用了两个collections，没用GridFS存大文件,Master-slave等高级特性也没设置过
